import json
import torch
from torch.utils.data import Dataset

class InputFeatures(object):
    def __init__(self,
                 input_tokens,
                 input_ids,
                 label,

    ):
        self.input_tokens = input_tokens
        self.input_ids = input_ids
        self.label=label

class ExploitsDataset(Dataset):
    def __init__(self, tokenizer, args, file_path=None):
        self.examples = []
        with open(file_path) as f:
            for line in f:
                js=json.loads(line.strip())
                self.examples.append(self.convert_examples_to_features(js,tokenizer,args))

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):       
        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)
    
    def convert_examples_to_features(self, js, tokenizer, args):
        code = js['code']
        code_tokens = tokenizer.tokenize(code)[:args['block_size']-2]
        source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]
        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)
        padding_length = args['block_size'] - len(source_ids)
        source_ids += [tokenizer.pad_token_id] * padding_length
        return InputFeatures(source_tokens, source_ids, js['label'])